{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: Adult <br>\n",
    "Obtained from: UCI Repository <br>\n",
    "Extraction was done by Barry Becker from the 1994 Census database.\n",
    "\n",
    "#### Parameters\n",
    "- age: the age of an individual\n",
    "- workclass: a general term to represent the employment status of an individual\n",
    "- fnlwgt: final weight. This is the number of people the census believes the entry represents..\n",
    "- education: the highest level of education achieved by an individual.\n",
    "- education­num: the highest level of education achieved in numerical form.\n",
    "- marital­status: marital status of an individual.\n",
    "- occupation: the general type of occupation of an individual\n",
    "- relationship: represents what this individual is relative to others.\n",
    "- race: Descriptions of an individual’s race\n",
    "- sex: the sex of the individual\n",
    "- capital­gain: capital gains for an individual\n",
    "- capital­loss: capital loss for an individual\n",
    "- hours­per­week: the hours an individual has reported to work per week\n",
    "- native­country: country of origin for an individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "#For library function for creating Pipeline and scaling data\n",
    "from sklearn.preprocessing import  StandardScaler  , LabelEncoder  \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# To split the data and evaluating the perfomance of the model\n",
    "from sklearn.model_selection import train_test_split,   cross_validate\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "#The models used are RandomForestClassifer and LogisticRegression and Catbboost\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Scoring methods used to evaluate the perfomance of the model\n",
    "from sklearn.metrics import f1_score \n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor \n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#For Resampling data\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.combine import SMOTEENN , SMOTETomek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn plotting style and context\n",
    "sns.set_theme(style='darkgrid')\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset from UCI database\n",
    "adult = fetch_ucirepo(id=2)\n",
    "\n",
    "# #Read the original dataset into a pandas dataframe\n",
    "df = adult.data.original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying information about the DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the float format for display purposes\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "# Displaying descriptive statistics of the DataFrame\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing any irregular values within the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting columns with object data type\n",
    "object_cols = df.select_dtypes('object').columns.to_list()\n",
    "\n",
    "# Displaying unique values in each object column\n",
    "for col in object_cols:\n",
    "    print(f'Unique values in {col} :\\n {df[col].unique()} \\n ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few issues here after observing the values in each feature & label <br>\n",
    "\n",
    "1. Our label has duplicated values as '<=50K' and '<=50K.' are supposed to be one and the same as well as '>50K.' & >50K'\n",
    "\n",
    "2. As we can see from the following columns as values '?' which should be better handled as NULL values :<br> \n",
    "    -  workclass \n",
    "    - occupation \n",
    "    - native-country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correcting label values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing income labels with binary values\n",
    "df.replace({'>50K': 1, '>50K.': 1, '<=50K': 0, '<=50K.': 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert all values from ? to NULL values\n",
    "df.replace('?' , np.nan ,inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values with most-frequent values\n",
    "values = {'workclass': 'Private', 'occupation': 'Prof-specialty', 'native-country': 'United-States'}\n",
    "df.fillna(value=values, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the 'education' column\n",
    "df.drop(['education' , 'fnlwgt'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the DataFrame\n",
    "copy = df.copy()\n",
    "\n",
    "# Extracting categorical columns for label encoding\n",
    "cat = copy.select_dtypes('object').columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating a LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Encoding labels in categorical columns\n",
    "for col in cat:\n",
    "    copy[col] = le.fit_transform(copy[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Variance Inflation Factor (VIF)\n",
    "def vif(dataframe):\n",
    "    vif_data = pd.DataFrame() \n",
    "    vif_data[\"feature\"] = dataframe.columns \n",
    "    \n",
    "    # Calculating VIF for each feature\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(dataframe.values, i) \n",
    "                            for i in range(len(dataframe.columns))] \n",
    "    return vif_data\n",
    "\n",
    "# Calculating VIF for the DataFrame\n",
    "vif(copy.drop('income' , axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get numerical and categorical columns\n",
    "def getColumnType(dataframe):\n",
    "    num_cols = dataframe.select_dtypes('number').columns.to_list()\n",
    "    cat_cols = dataframe.select_dtypes('object').columns.to_list()\n",
    "    return(num_cols, cat_cols)\n",
    "\n",
    "# Getting numerical and categorical columns\n",
    "num_cols, cat_cols = getColumnType(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating correlation matrix\n",
    "corr = df[num_cols].corr()\n",
    "\n",
    "# Plotting the heatmap of correlation matrix\n",
    "sns.heatmap(corr, annot=True, mask=np.triu(corr))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Kernel Density Estimates for numerical features\n",
    "fig, ax = plt.subplots(nrows=2, ncols=3)\n",
    "plt.suptitle('Kernel Density Estimates for Various Features', fontsize=16)\n",
    "\n",
    "for i, col in enumerate(num_cols, 0):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    sns.kdeplot(df[col], color='orange', fill=True)\n",
    "\n",
    "ax[1,2].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting class balance visualization\n",
    "ax = sns.countplot(x='income', data=df, hue='income', palette='pastel', stat='percent')\n",
    "ax.xaxis.set_ticks(ax.get_xticks())\n",
    "ax.set_xticklabels(['<=50K', \">50K\"])\n",
    "ax.legend(labels=['<=50K', \">50K\"])\n",
    "ax.set_title('Class Balance Visualization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating baseline models\n",
    "\n",
    "Algorithms used: \n",
    "\n",
    "1) Decision Tree\n",
    "2) KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing features and target variables\n",
    "features = df.iloc[:, :-1]\n",
    "target = df['income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding categorical features\n",
    "features = pd.get_dummies(features, columns=cat_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance of DecisionTreeClassifier and fitting the model\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(x_train, y_train)\n",
    "\n",
    "# Making predictions without feature scaling and calculating F1 score\n",
    "wo_scale_pred = dt.predict(x_test)\n",
    "f1_score(y_test, wo_scale_pred, average='weighted')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the features using StandardScaler\n",
    "ss = StandardScaler()\n",
    "x_train_scale = ss.fit_transform(x_train)\n",
    "x_test_scale = ss.fit_transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the DecisionTreeClassifier model on standardized features and calculating F1 score\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(x_train_scale, y_train)\n",
    "wo_scale_pred = dt.predict(x_test_scale)\n",
    "f1_score(y_test, wo_scale_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining the alphas for cost complexity pruning\n",
    "path = dt.cost_complexity_pruning_path(x_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ccp_alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DecisionTreeClassifier models with different ccp alphas\n",
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas[100:]:\n",
    "    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha , class_weight='balanced')\n",
    "    clf.fit(x_train, y_train)\n",
    "    clfs.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on training and testing sets using different models\n",
    "train_pred = [clf.predict(x_train) for clf in clfs]\n",
    "test_pred = [clf.predict(x_test) for clf in clfs]\n",
    "\n",
    "# Calculating F1 scores for each model\n",
    "train_scores = [f1_score(y_train, pred, average='weighted') for pred in train_pred]\n",
    "test_scores = [f1_score(y_test, pred, average='weighted') for pred in test_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "# Add training data trace\n",
    "fig.add_trace(go.Scatter(x=ccp_alphas[100:2200], y=train_scores, name='Train'))\n",
    "\n",
    "# Add testing data trace\n",
    "fig.add_trace(go.Scatter(x=ccp_alphas[100:2200], y=test_scores, name='Test'))\n",
    "fig.update_layout(xaxis_tickformat=\".5f\")\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"alpha\",\n",
    "    yaxis_title=\"accuracy\",\n",
    "    title=\"Weighted F1 vs alpha for training and testing sets\"\n",
    ")\n",
    "\n",
    "fig.add_annotation(x=0.000145,y=0.826 ,\n",
    "            text=\"alpha = 0.000145\",\n",
    "            showarrow=True,\n",
    "            font_size = 13)\n",
    "\n",
    "        \n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=42, ccp_alpha=0.000145)\n",
    "\n",
    "# Fitting the model on the training data\n",
    "dt.fit(x_train, y_train)\n",
    "\n",
    "# Making predictions on the test data\n",
    "test_pred = dt.predict(x_test)\n",
    "train_pred = dt.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print the F1 scores for the training and test sets\n",
    "def print_scores(actual_train, actual_test, train_predict, test_pred):\n",
    "    train_score = f1_score(actual_train, train_predict, average='weighted')\n",
    "    test_score = f1_score(actual_test, test_pred, average='weighted')\n",
    "    print(\"Train F1 Score:\", train_score)\n",
    "    print(\"Test F1 Score:\", test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the F1 scores for the training and test sets\n",
    "print_scores(y_train, y_test, train_pred, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does different techniques affect our imbalaned datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "features = df.iloc[:,:-1]\n",
    "target = df['income']\n",
    "features = pd.get_dummies(features)\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svc_model(xtrain, ytrain, xtest, ytest):\n",
    "    # Create a pipeline including standard scaling and SVC with balanced class weights\n",
    "    pipeline = Pipeline([('scaling', StandardScaler()), ('svc', SVC(class_weight='balanced'))])\n",
    "    \n",
    "    # Fit the pipeline on the training data\n",
    "    pipeline.fit(xtrain, ytrain)\n",
    "    \n",
    "    # Predict on the test data\n",
    "    baseline = pipeline.predict(xtest)\n",
    "    \n",
    "    # Calculate and return the weighted F1 score\n",
    "    return f1_score(ytest, baseline, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the svc_model function and print the F1 score\n",
    "svc_model(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling \n",
    " - SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting features from the DataFrame except for the last column\n",
    "features = df.iloc[:, :-1]\n",
    "\n",
    "# Extracting the target variable from the DataFrame\n",
    "target = df['income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing SMOTENC oversampling technique with specified categorical features and random state\n",
    "smotenc = SMOTENC(categorical_features=[1, 3, 4, 5, 6, 7, 11], random_state=42)\n",
    "\n",
    "# Fitting the predictor and target variables to generate synthetic samples using SMOTENC\n",
    "x_smote, y_smote = smotenc.fit_resample(features, target)\n",
    "\n",
    "# Performing one-hot encoding on the resampled features\n",
    "x_smote = pd.get_dummies(x_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the resampled data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_smote, y_smote, test_size=0.3, random_state=42)\n",
    "\n",
    "# Calling the svc_model function to train and evaluate the SVC model\n",
    "svc_model(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UnderSampling\n",
    "- NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing NearMiss undersampling technique with specified number of neighbors\n",
    "nm = NearMiss(n_neighbors=3)\n",
    "\n",
    "# Performing one-hot encoding on the features\n",
    "features = pd.get_dummies(features)\n",
    "\n",
    "# Fitting the NearMiss undersampling technique to the data to balance the target variable\n",
    "x_nm, y_nm = nm.fit_resample(features, target)\n",
    "\n",
    "# Splitting the resampled data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_nm, y_nm, test_size=0.3, random_state=42)\n",
    "\n",
    "# Calling the svc_model function to train and evaluate the SVC model\n",
    "svc_model(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined OverSampling and UnderSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting features and target variable from the DataFrame\n",
    "features = df.iloc[:, :-1]\n",
    "target = df['income']\n",
    "\n",
    "# Performing one-hot encoding on categorical columns\n",
    "features = pd.get_dummies(features, columns=cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing SMOTEENN and SMOTETomek resampling techniques\n",
    "sme = SMOTEENN(random_state=42)\n",
    "smt = SMOTETomek(random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling the data using SMOTEENN and SMOTETomek techniques\n",
    "x_sme, y_sme = sme.fit_resample(features, target)\n",
    "x_smt, y_smt = smt.fit_resample(features, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the resampled data into training and test sets for SMOTEENN\n",
    "x_train_sme, x_test_sme, y_train_sme, y_test_sme = train_test_split(x_sme, y_sme, test_size=0.3, random_state=42)\n",
    "\n",
    "# Splitting the resampled data into training and test sets for SMOTETomek\n",
    "x_train_smt, x_test_smt, y_train_smt, y_test_smt = train_test_split(x_smt, y_smt, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the svc_model function to train and evaluate the SVC model on SMOTEENN resampled data\n",
    "svc_model(x_train_sme, y_train_sme, x_test_sme, y_test_sme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the svc_model function to train and evaluate the SVC model on SMOTETomek resampled data\n",
    "svc_model(x_train_smt, y_train_smt, x_test_smt, y_test_smt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visulise_noise(X1, X2, y1, y2):\n",
    "    # Fit PCA pipeline\n",
    "    pipeline = Pipeline([('scaling', StandardScaler()), ('pca', PCA(n_components=3))])\n",
    "    features_pca1 = pipeline.fit_transform(X1)\n",
    "    features_pca2 = pipeline.transform(X2)\n",
    "\n",
    "    # Create DataFrames for visualization\n",
    "    pc1 = pd.DataFrame(features_pca1, columns=['pc1', 'pc2', 'pc3'])\n",
    "    pc2 = pd.DataFrame(features_pca2, columns=['pc1', 'pc2', 'pc3'])\n",
    "\n",
    "    pc1['targets'] = y1\n",
    "    pc2['targets'] = y2\n",
    "\n",
    "    # Create subplots\n",
    "    fig = make_subplots(rows=1, cols=2, specs=[[{'type': 'scatter3d'}, {'type': 'scatter3d'}]])\n",
    "\n",
    "    # Add 3D scatterplots to subplots\n",
    "    fig.add_annotation(text=f\"Baseline\", xref=\"paper\", yref=\"paper\", x=0.10, y=0.97, showarrow=False)\n",
    "    fig.add_trace(go.Scatter3d(x=pc1['pc1'], y=pc1['pc2'], z=pc1['pc3'], marker=dict(color=pc1['targets']), mode='markers', showlegend=False), row=1, col=1)\n",
    "\n",
    "    fig.add_annotation(text=f\"SMOTEENN\", xref=\"paper\", yref=\"paper\", x=0.65, y=0.97, showarrow=False)\n",
    "    fig.add_trace(go.Scatter3d(x=pc2['pc1'], y=pc2['pc2'], z=pc2['pc3'], marker=dict(color=pc2['targets']), mode='markers', showlegend=False), row=1, col=2)\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visulise_noise(features , x_sme , target , y_sme)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
